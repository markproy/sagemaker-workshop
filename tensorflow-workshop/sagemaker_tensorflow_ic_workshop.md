# Amazon SageMaker - TensorFlow / Keras Workshop using Image Classification

[Amazon SageMaker](https://aws.amazon.com/sagemaker/) provides every developer and data scientist the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow to label and prepare your data, choose an algorithm, train the model, tune and optimize it for deployment, make predictions, and take action. Your models get to production faster with much less effort and lower cost.

This workshop provides a set of lab exercises that demonstrate how machine learning developers and data scientists can leverage SageMaker to train, build, optimize, and deploy TensorFlow models using Keras. [TensorFlow, Keras and SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html) can be used for a wide variety of machine learning use cases. This workshop uses an Image Classification problem throughout the series of labs.

## Labs

### Using TensorFlow natively in SageMaker Jupyter notebook
This [introductury lab](./1_tf_image_classification_birds.ipynb) introduces the bird species image classification problem and acts as a **baseline of using Amazon SageMaker with TensorFlow**. The lab walks through preparing the data, and training an image classifier using transfer learning from a pretrained MobileNet model. It also evaluates the model. All this is done in the context of a SageMaker hosted Jupyter notebook. For someone familiar with TensorFlow and Keras, this is a straightforward first step of an end to end process directly from a SageMaker notebook.

### Leveraging SageMaker training and hosting with TensorFlow
Building on the introductory lab, [this lab](./2_tf_sm_image_classification_birds.ipynb) demonstrates the use of **SageMaker's TensorFlow container**. It shows that migration of existing TensorFlow models into the SageMaker platform is straightforward. SageMaker's managed training service is used to create training jobs on demand, reducing training cost by not paying for idle time. Similarly, the lab demonstrates deployment of TensorFlow models behind a managed endpoint, providing an easy to use, scalable and cost effective approach to on demand predictions. Click [here](https://sagemaker.readthedocs.io/en/stable/using_tf.html) for details of using the container.

### Making batch predictions with SageMaker and TensorFlow
[This lab](./3_tf_sm_batch_ic.ipynb) shows how to make **batch predictions with TensorFlow on SageMaker**. Many customers have machine learning workloads that require a large number of predictions to be made reliably on a repeatable schedule. As compared to SageMaker's managed hosting service, inference compute capacity for batch predictions is spun up on demand and taken down upon completion of the batch. For large batch workloads, this represents significant cost savings over an always-on endpoint. Data scientists can stay focused on creating the best models, since [SageMaker batch](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) uses the same trained model easily across hosted endpoints and batch.

### Automatic model tuning of TensorFlow models with SageMaker
[This lab](./4_tf_sm_auto_model_tuning.ipynb) demonstrates the power of **Amazon SageMaker's automatic model tuning capability**, also known as hyperparameter optimization (HPO). Instead of a labor intensive process of trial and error that could take days or weeks, [automatic model tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) let's a data scientist ask SageMaker to find the optimal set of hyperparameters. The notebook shows how to provide a set of parameters to tune and ranges to consider, a metric to optimize on, some limits on the number of jobs to consider and the compute capacity to leverage. A SageMaker tuning job then efficiently explores options using a Bayesian optimization. SageMaker creates a set of models and highlights which one is optimal given your constraints. The resulting model is ready for deployment behind an endpoint or for batch predictions.

### Leveraging multi-GPU instance types for faster TensorFlow training on SageMaker
This lab shows the power of **Amazon SageMaker's GPU infrastructure** for speeding up machine learning workloads. With a small change to your Keras / TensorFlow script, a SageMaker training job can leverage multiple GPU's to significantly reduce your training job duration on a single powerful instance. SageMaker provides the `ml.p3.8xlarge` with 4 GPU's or the `ml.p3.16xlarge` with 8 GPU's. Click [here](https://aws.amazon.com/sagemaker/pricing/instance-types/) for the latest specifications of SageMaker's supported instance types.