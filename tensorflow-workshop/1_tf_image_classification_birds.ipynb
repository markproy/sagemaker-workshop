{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Classification with Tensorflow on Amazon SageMaker - Directly in your notebook\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "3. [Train the model](#Train-the-model)\n",
    "4. [Test the model](#Test-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Image classification is an increasingly popular machine learning technique, in which a trained model predicts which of several classes is represented by a particular image. This technique is useful across a wide variety of use cases from manufacturing quality control to medical diagnosis. To create an image classification solution, we need to acquire and process an image dataset, and train a model from that dataset. The trained model is then capable of identifying features and predicting which class an image belongs to. Finally, we can make predictions using the trained model against previously unseen images.\n",
    "\n",
    "This notebook is an end-to-end example showing how to build an image classifier using TensorFlow and Keras, simply using Amazon SageMaker's hosted Jupyter notebook directly. This is an easy transition from traditional machine learning development you may already be doing on your laptop or on an Amazon EC2 instance. Subsequent notebooks in this workshop will demonstrate how to take full advantage of SageMaker's training service, hosting service, and automatic model tuning. Note that for complex large scale machine learning models, training directly in a notebook can be cost prohibitive.\n",
    "\n",
    "For each of the labs in this workshop, we use a publicly available set of bird images based on the [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset. We demonstrate transfer learning by leveraging pretrained ImageNet weights for a MobileNet V2 network architecture.\n",
    "\n",
    "For a quick demonstration, pick a small handful of bird species (set `SAMPLE_ONLY = True` and choose a few classes / species). For a more complete model, you can train against all 200 bird species in the dataset. For anything more than a few classes, be sure to upgrade your notebook instance type to one of SageMaker's GPU instance types (ml.p2, ml.p3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset contains 11,788 images across 200 bird species (the original technical report can be found [here](http://www.vision.caltech.edu/visipedia/papers/CUB_200_2011.pdf)).  Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels.  Bounding boxes are provided, as are annotations of bird parts.  A recommended train/test split is given, but image size data is not.\n",
    "\n",
    "![](./cub_200_2011_snapshot.png)\n",
    "\n",
    "The dataset can be downloaded [here](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html).\n",
    "\n",
    "### Download and unpack the dataset\n",
    "\n",
    "Here we download the birds dataset from CalTech. You can do this once and keep the unpacked dataset in your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split('/')[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "download('http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Clean up prior version of the downloaded dataset if you are running this again\n",
    "!rm -rf CUB_200_2011  \n",
    "\n",
    "# Unpack and then remove the downloaded compressed tar file\n",
    "!gunzip -c ./CUB_200_2011.tgz | tar xopf - \n",
    "!rm CUB_200_2011.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some parameters for the rest of the notebook to use\n",
    "Here we define a few parameters that help drive the rest of the notebook.  For example, `SAMPLE_ONLY` is defaulted to `True`. This will force the notebook to train on only a handful of species.  Setting `SAMPLE_ONLY` to false will make the notebook work with the entire dataset of 200 bird species.  This makes the training a more difficult challenge, and you will need to tune parameters and run more epochs.\n",
    "\n",
    "An `EXCLUDE_IMAGE_LIST` is defined as a mechanism to address any corrupt images from the dataset and ensure they do not disrupt the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To speed up training and experimenting, you can use a small handful of species.\n",
    "# To see the full list of the classes available, look at the content of CLASSES_FILE.\n",
    "SAMPLE_ONLY  = True\n",
    "CLASSES = [13, 17] #, 35, 36, 47, 68, 73, 87]\n",
    "\n",
    "# Otherwise, you can use the full set of species\n",
    "if (not SAMPLE_ONLY):\n",
    "    CLASSES = []\n",
    "    for c in range(200):\n",
    "        CLASSES += [c + 1]\n",
    "\n",
    "BASE_DIR   = 'CUB_200_2011/'\n",
    "IMAGES_DIR = BASE_DIR + 'images/'\n",
    "\n",
    "CLASSES_FILE = BASE_DIR + 'classes.txt'\n",
    "IMAGE_FILE   = BASE_DIR + 'images.txt'\n",
    "LABEL_FILE   = BASE_DIR + 'image_class_labels.txt'\n",
    "\n",
    "SPLIT_RATIOS = (0.7, 0.2, 0.1)\n",
    "\n",
    "CLASS_COLS      = ['class_number','class_id']\n",
    "\n",
    "EXCLUDE_IMAGE_LIST = ['087.Mallard/Mallard_0130_76836.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the dataset\n",
    "Show the list of bird species or dataset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.read_csv(CLASSES_FILE, sep=' ', names=CLASS_COLS, header=None)\n",
    "criteria = classes_df['class_number'].isin(CLASSES)\n",
    "classes_df = classes_df[criteria]\n",
    "\n",
    "class_name_list = sorted(classes_df['class_id'].unique().tolist())\n",
    "print(class_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each species, there are dozens of images of various shapes and sizes. By dividing the entire dataset into individual named (numbered) folders, the images are in effect labelled for supervised learning using image classification and object detection algorithms. \n",
    "\n",
    "The following function displays a grid of thumbnail images for all the image files for a given species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_species(species_id):\n",
    "    _im_list = !ls $IMAGES_DIR/$species_id\n",
    "\n",
    "    NUM_COLS = 4\n",
    "    IM_COUNT = len(_im_list)\n",
    "\n",
    "    print('Species ' + species_id + ' has ' + str(IM_COUNT) + ' images.')\n",
    "    \n",
    "    NUM_ROWS = int(IM_COUNT / NUM_COLS)\n",
    "    if ((IM_COUNT % NUM_COLS) > 0):\n",
    "        NUM_ROWS += 1\n",
    "\n",
    "    fig, axarr = plt.subplots(NUM_ROWS, NUM_COLS)\n",
    "    fig.set_size_inches(12.0, 20.0, forward=True)\n",
    "\n",
    "    curr_row = 0\n",
    "    for curr_img in range(IM_COUNT):\n",
    "        # fetch the url as a file type object, then read the image\n",
    "        f = IMAGES_DIR + species_id + '/' + _im_list[curr_img]\n",
    "        a = plt.imread(f)\n",
    "\n",
    "        # find the column by taking the current index modulo 3\n",
    "        col = curr_img % NUM_ROWS\n",
    "        # plot on relevant subplot\n",
    "        axarr[col, curr_row].imshow(a)\n",
    "        if col == (NUM_ROWS - 1):\n",
    "            # we have finished the current row, so increment row counter\n",
    "            curr_row += 1\n",
    "\n",
    "    fig.tight_layout()       \n",
    "    plt.show()\n",
    "        \n",
    "    # Clean up\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_species('013.Bobolink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train/val/test dataframes from our dataset\n",
    "Here we split our dataset into training, testing, and validation datasets, each in their own Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_train_val_test(df, label_column, splits=(0.7, 0.2, 0.1), verbose=False):\n",
    "    train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    labels = df[label_column].unique()\n",
    "    for lbl in labels:\n",
    "        lbl_df = df[df[label_column] == lbl]\n",
    "\n",
    "        lbl_train_df        = lbl_df.sample(frac=splits[0])\n",
    "        lbl_val_and_test_df = lbl_df.drop(lbl_train_df.index)\n",
    "        lbl_test_df         = lbl_val_and_test_df.sample(frac=splits[2]/(splits[1] + splits[2]))\n",
    "        lbl_val_df          = lbl_val_and_test_df.drop(lbl_test_df.index)\n",
    "\n",
    "        if verbose:\n",
    "            print('\\n{}:\\n---------\\ntotal:{}\\ntrain_df:{}\\nval_df:{}\\ntest_df:{}'.format(lbl,\n",
    "                                                                        len(lbl_df), \n",
    "                                                                        len(lbl_train_df), \n",
    "                                                                        len(lbl_val_df), \n",
    "                                                                        len(lbl_test_df)))\n",
    "        train_df = train_df.append(lbl_train_df)\n",
    "        val_df   = val_df.append(lbl_val_df)\n",
    "        test_df  = test_df.append(lbl_test_df)\n",
    "\n",
    "    # shuffle them on the way out using .sample(frac=1)\n",
    "    return train_df.sample(frac=1), val_df.sample(frac=1), test_df.sample(frac=1)\n",
    "\n",
    "def get_train_val_dataframes():\n",
    "    images_df = pd.read_csv(IMAGE_FILE, sep=' ',\n",
    "                            names=['image_pretty_name', 'image_file_name'],\n",
    "                            header=None)\n",
    "    image_class_labels_df = pd.read_csv(LABEL_FILE, sep=' ',\n",
    "                                names=['image_pretty_name', 'orig_class_id'], header=None)\n",
    "\n",
    "    # Merge the metadata into a single flat dataframe for easier processing\n",
    "    full_df = pd.DataFrame(images_df)\n",
    "    full_df = full_df[~full_df.image_file_name.isin(EXCLUDE_IMAGE_LIST)]\n",
    "\n",
    "    full_df.reset_index(inplace=True, drop=True)\n",
    "    full_df = pd.merge(full_df, image_class_labels_df, on='image_pretty_name')\n",
    "\n",
    "    if SAMPLE_ONLY:\n",
    "        # grab a small subset of species for testing\n",
    "        criteria = full_df['orig_class_id'].isin(CLASSES)\n",
    "        full_df = full_df[criteria]\n",
    "        print('Using subset of total images based on sample class list. subtotal: {}'.format(full_df.shape[0]))\n",
    "\n",
    "    unique_classes = full_df['orig_class_id'].drop_duplicates()\n",
    "    sorted_unique_classes = sorted(unique_classes)\n",
    "    id_to_one_based = {}\n",
    "    i = 1\n",
    "    for c in sorted_unique_classes:\n",
    "        id_to_one_based[c] = str(i)\n",
    "        i += 1\n",
    "\n",
    "    full_df['class_id'] = full_df['orig_class_id'].map(id_to_one_based)\n",
    "    full_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    def get_class_name(fn):\n",
    "        return fn.split('/')[0]\n",
    "    full_df['class_name'] = full_df['image_file_name'].apply(get_class_name)\n",
    "    full_df = full_df.drop(['image_pretty_name'], axis=1)\n",
    "\n",
    "    train_df = []\n",
    "    test_df  = []\n",
    "    val_df   = []\n",
    "\n",
    "    # split into training and validation sets\n",
    "    train_df, val_df, test_df = split_to_train_val_test(full_df, 'class_id', SPLIT_RATIOS)\n",
    "\n",
    "    print('num images total: ' + str(images_df.shape[0]))\n",
    "    print('\\nnum train: ' + str(train_df.shape[0]))\n",
    "    print('num val: ' + str(val_df.shape[0]))\n",
    "    print('num test: ' + str(test_df.shape[0]))\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = get_train_val_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model \n",
    "Here we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 224\n",
    "WIDTH  = 224\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "LAST_FROZEN_LAYER = 20\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare image data generators from our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen =  ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input,\n",
    "      rotation_range=60,\n",
    "      brightness_range=(0.8, 1.0),\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=False\n",
    "    )\n",
    "val_datagen  = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(train_df, directory=IMAGES_DIR,\n",
    "                                              x_col='image_file_name', y_col='class_id',\n",
    "                                              target_size=(HEIGHT, WIDTH), \n",
    "                                              batch_size=BATCH_SIZE)\n",
    "val_gen = train_datagen.flow_from_dataframe(val_df, directory=IMAGES_DIR,\n",
    "                                              x_col='image_file_name', y_col='class_id',\n",
    "                                              target_size=(HEIGHT, WIDTH), \n",
    "                                              batch_size=BATCH_SIZE)\n",
    "test_gen = train_datagen.flow_from_dataframe(test_df, directory=IMAGES_DIR,\n",
    "                                              x_col='image_file_name', y_col='class_id',\n",
    "                                              target_size=(HEIGHT, WIDTH), \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False) # need predictable order for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(HEIGHT, WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    # Freeze all base layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        x = Dense(fc, activation='relu')(x) \n",
    "        if (dropout != 0.0):\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "    # New softmax layer\n",
    "    predictions = Dense(num_classes, activation='softmax', name='output')(x) \n",
    "    \n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return finetune_model\n",
    "\n",
    "# Here we extend the base model with additional fully connected layers, dropout for avoiding\n",
    "# overfitting to the training dataset, and a classification layer\n",
    "num_classes = len(class_name_list)\n",
    "model = build_finetune_model(base_model, \n",
    "                              dropout=0.5, \n",
    "                              fc_layers=[1024], \n",
    "                              num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "INITIAL_EPOCHS = 2\n",
    "FINE_TUNING = True\n",
    "\n",
    "num_train_images = len(train_gen.filepaths)\n",
    "num_val_images   = len(val_gen.filepaths)\n",
    "\n",
    "opt = RMSprop(lr=0.00001) # or Adam\n",
    "\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "if not FINE_TUNING:\n",
    "    history = model.fit_generator(train_gen, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       validation_data=val_gen, validation_steps=num_val_images // BATCH_SIZE,\n",
    "                                       shuffle=True)\n",
    "else:\n",
    "    model.fit_generator(train_gen, epochs=INITIAL_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       validation_data=val_gen, validation_steps=num_val_images // BATCH_SIZE,\n",
    "                                       shuffle=True)\n",
    "    \n",
    "    for layer in model.layers[LAST_FROZEN_LAYER:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit_generator(train_gen, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       validation_data=val_gen, validation_steps=num_val_images // BATCH_SIZE,\n",
    "                                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')\n",
    "model.save('./checkpoints/' + 'MobileNetV2' + '_bird_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./checkpoints/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss across epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'r.')\n",
    "    plt.plot(epochs, val_acc, 'r')\n",
    "    plt.title('Training accuracy')\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.plot(epochs, loss, 'r.')\n",
    "    # plt.plot(epochs, val_loss, 'r-')\n",
    "    # plt.title('Training and validation loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig('acc_vs_epochs.png')\n",
    "    \n",
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_preds = model.evaluate_generator(test_gen, steps=test_df.shape[0])\n",
    "print('Loss: {:.2f}, Accuracy: {:.2f}'.format(eval_preds[0], eval_preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "def predict_bird_from_file(fn, verbose=True):\n",
    "    img = image.load_img(fn, target_size=(HEIGHT, WIDTH))\n",
    "    x = image.img_to_array(img)\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    results = model.predict(x)\n",
    "    predicted_class_idx = argmax(results)\n",
    "    predicted_class = class_name_list[predicted_class_idx]\n",
    "    confidence = results[0][predicted_class_idx]\n",
    "    if verbose:\n",
    "        display(Image(fn))\n",
    "        print('Class: {}, confidence: {:.2f}'.format(predicted_class, confidence))\n",
    "    del img, x\n",
    "    return predicted_class_idx, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = IMAGES_DIR + '/' + test_df.iloc[0]['image_file_name']\n",
    "predict_bird_from_file(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), \n",
    "                                  range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.gca().set_xticklabels(class_name_list)\n",
    "    plt.gca().set_yticklabels(class_name_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def create_and_plot_confusion_matrix(actual, predicted):\n",
    "    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(class_name_list)))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=range(len(class_name_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Iterate through entire dataframe, tracking predictions and accuracy. For mistakes, show the image, and the predicted and actual classes to help understand\n",
    "# where the model may need additional tuning.\n",
    "\n",
    "def test_image_df(df):\n",
    "    print('Testing {} images'.format(df.shape[0]))\n",
    "    num_errors = 0\n",
    "    preds = []\n",
    "    acts  = []\n",
    "    for i in range(df.shape[0]):\n",
    "        fname = df.iloc[i]['image_file_name']\n",
    "        act   = int(df.iloc[i]['class_id']) - 1\n",
    "        acts.append(act)\n",
    "        pred, conf = predict_bird_from_file(IMAGES_DIR + '/' + fname, verbose=False)\n",
    "        preds.append(pred)\n",
    "        if (pred != act):\n",
    "            num_errors += 1\n",
    "            print('ERROR on image index {} -- Pred: {} {:.2f}, Actual: {}'.format(i, \n",
    "                                                                   class_name_list[pred], conf, \n",
    "                                                                   class_name_list[act]))\n",
    "            display(Image(filename=IMAGES_DIR + '/' + fname))\n",
    "    return num_errors, preds, acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess prediction performance against validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = val_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(val_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = test_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(test_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively use the Keras predict_generator for dataset evaluation\n",
    "Is convenient, but doesn't easily give access to the prediction mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen.reset()\n",
    "test_preds = model.predict_generator(test_gen, steps=test_df.shape[0], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(test_preds, axis=1)\n",
    "acts  = np.asarray(test_gen.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model against previously unseen images\n",
    "Here we download images that the algorithm has not yet seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O northern-cardinal-2.jpg https://www.allaboutbirds.org/guide/assets/photo/63667291-480px.jpg\n",
    "!wget -q -O bobolink-1.jpg https://upload.wikimedia.org/wikipedia/commons/6/6b/Bobolink%2C_Mer_Bleue.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bird_from_file('northern-cardinal-2.jpg')\n",
    "predict_bird_from_file('bobolink-1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
