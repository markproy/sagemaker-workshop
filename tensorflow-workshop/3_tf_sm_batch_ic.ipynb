{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making batch predictions using a TensorFlow model with Amazon SageMaker\n",
    "This notebook shows how to make **batch predictions with TensorFlow on SageMaker**. Many customers have machine learning workloads that require a large number of predictions to be made reliably on a repeatable schedule. As compared to SageMaker's managed hosting service, compute capacity for batch predictions is spun up on demand and taken down upon completion of the batch. For large batch workloads, this represents significant cost savings over an always-on endpoint. \n",
    "\n",
    "Another benefit of SageMaker batch is that it allows data scientists can stay focused on creating the best models.\n",
    "[SageMaker batch](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) uses the same trained model easily across hosted endpoints and batch, with no need for expensive rewrites or infrastructure. Here is an overview picture showing how clusters of batch transformer instances are able to make predictions at scale. You provide the input via s3, and SageMaker returns the predictions via s3 as well. Note that the same input and output handlers you used for your SageMaker endpoint in the [previous lab](./2_tf_sm_image_classification_birds.ipynb) are used for batch predictions. Likewise, the same trained model works for both. TensorFlow Serving is leveraged in both cases.\n",
    "\n",
    "![](./batch_overview.png)\n",
    "\n",
    "Here is a [blog post](https://aws.amazon.com/blogs/machine-learning/performing-batch-inference-with-tensorflow-serving-in-amazon-sagemaker/) with additional detail on performing batch TensorFlow predictions with SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This notebook assumes you have already trained your TensorFlow model in the prior lab, which results in model artifacts being available in S3. Update the `training_job_name` variable below to refer to your specific training job, so that the notebook has a full s3 URI to the model artifacts. \n",
    "\n",
    "These same model artifacts were used for deployment in a SageMaker hosted endpoint in the previous lab. In this lab, we demonstrate batch predictions with the same trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import tensorflow\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from time import gmtime, strftime\n",
    "\n",
    "training_job_name = 'mpr-tf-ic-2019-09-10-12-00-28-178'  ### Replace this with your job name from the previous lab\n",
    "\n",
    "USE_GPU_INSTANCE     = True\n",
    "TF_FRAMEWORK_VERSION = '1.14'\n",
    "\n",
    "if TF_FRAMEWORK_VERSION == '1.12':\n",
    "    TF_ACCOUNT_NUM = '520713654638'\n",
    "    TF_CONTAINER_NAME = 'sagemaker-tensorflow-serving'\n",
    "else:\n",
    "    TF_ACCOUNT_NUM = '763104351884'\n",
    "    TF_CONTAINER_NAME = 'tensorflow-inference'\n",
    "## Here is the documentation for identifying TensorFlow SageMaker container images\n",
    "##   https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with evaluating the batch prediction results, enter the list of class labels that your classifier was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name_list = ['013.Bobolink', '017.Cardinal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model for performing batch predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deployed the model in the previous lab to an Amazon SageMaker real time endpoint, we deployed to a CPU-based instance type.  Under the hood a CPU-based Amazon SageMaker Model object was created to wrap a CPU-based TFS container.  However, for Batch Transform on a large dataset, we would prefer to use full GPU instances.  To do this, we need to create another Model object that will utilize a GPU-based TFS container.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate a Model object pointing to the trained model artifacts and referring to the TensorFlow Serving image that will be used to drive inference on that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'DEMO-TF-image-classification-birds'\n",
    "model_prefix = 'mpr-tf-ic-gpu'\n",
    "\n",
    "model_artifacts = 's3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name)\n",
    "print(model_artifacts)\n",
    "\n",
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "\n",
    "model_name = '{}-{}'.format(model_prefix, strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "\n",
    "if USE_GPU_INSTANCE:\n",
    "    batch_instance_type = 'ml.p3.2xlarge'\n",
    "    framework_image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}-gpu'.format(TF_ACCOUNT_NUM,\n",
    "                                                                     region_name, TF_CONTAINER_NAME,\n",
    "                                                                     TF_FRAMEWORK_VERSION)\n",
    "    print('Ensuring tensorflow-gpu Python package is installed for batch inference')\n",
    "    # publish a model_gpu.tar.gz with an appropriate requirements.txt for running on a gpu instance\n",
    "    model_artifacts_base = model_artifacts[0:model_artifacts.index('model.tar.gz') - 1] # no slash at end\n",
    "    !bash ./replace-requirements-gpu.sh $model_artifacts_base\n",
    "    model_artifacts = model_artifacts_base + '/model_gpu.tar.gz'\n",
    "else:\n",
    "    batch_instance_type = 'ml.c5.4xlarge'\n",
    "    framework_image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}-cpu'.format(TF_ACCOUNT_NUM,\n",
    "                                                                     region_name, TF_CONTAINER_NAME,\n",
    "                                                                     TF_FRAMEWORK_VERSION)\n",
    "print('Using TensorFlow Serving image: {}'.format(framework_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_serving_model = Model(model_data=model_artifacts,\n",
    "                         role=sagemaker.get_execution_role(),\n",
    "                         image=framework_image,\n",
    "                         framework_version=TF_FRAMEWORK_VERSION,\n",
    "                         sagemaker_session=sess)\n",
    "\n",
    "tf_serving_container = tf_serving_model.prepare_container_def(batch_instance_type)\n",
    "model_params = {\n",
    "    'ModelName': model_name,\n",
    "    'Containers': [\n",
    "        tf_serving_container\n",
    "    ],\n",
    "    'ExecutionRoleArn': sagemaker.get_execution_role()\n",
    "}\n",
    "\n",
    "client.create_model(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker batch transformations require input to be specified in s3, and you need to provide an s3 output path where SageMaker will save the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path  = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "output_data_path = 's3://{}/{}/batch_predictions'.format(bucket, prefix)\n",
    "print(output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the batch transformation, we first remove prior batch prediction results. In production, you would likely instead tag the folder with a timestamp and retain the results from each run of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input('Are you sure you want to remove the old batch predictions {}?'.format(output_data_path)) == 'yes':\n",
    "    !aws s3 rm --quiet --recursive $output_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, to interpret the results, we copy them down to our local folder. If we have done this before, we first remove the old results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input('Are you sure you want to remove the prior local batch predictions from ./batch_predictions') == 'yes':\n",
    "    !rm -rf ./batch_predictions/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the batch transformation job\n",
    "Here we kick off the batch prediction job using the SageMaker Transformer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_instance_count = 2\n",
    "concurrency = 100\n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    model_name = model_name,\n",
    "    instance_count = batch_instance_count,\n",
    "    instance_type = batch_instance_type,\n",
    "    max_concurrent_transforms = concurrency,\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='tf-birds-image-transform')\n",
    "\n",
    "transformer.transform(data = input_data_path, content_type = 'application/x-image')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate evaluation of the output, we download the results to our local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp  --quiet --recursive $output_data_path ./batch_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take a look at a sample output file. For each jpg file we passed to the batch transformation job, we get a corresponding `.jpg.out` file containing the json formatted output from the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filepaths = glob.glob('./batch_predictions/*/*')\n",
    "sample_file = filepaths[0]\n",
    "!cat $sample_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the highest probability class prediction for each image and compare that to the actual class of the image (represented by its class subfolder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for entry in glob.glob('batch_predictions/*/*'):\n",
    "    try:\n",
    "        actual_label = entry.split('/')[1]\n",
    "        actual_index = class_name_list.index(actual_label)\n",
    "        with open(entry, 'r') as f:\n",
    "            jstr = json.load(f)\n",
    "            results = [float('%.3f'%(item)) for sublist in jstr['predictions'] for item in sublist]\n",
    "            class_index = np.argmax(np.array(results))\n",
    "            predicted_label = class_name_list[class_index]\n",
    "            predicted.append(class_index)\n",
    "            actual.append(actual_index)\n",
    "            is_correct = (predicted_label == actual_label) or False\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Out of {} total images, accurate predictions were returned for {}'.format(total, correct))\n",
    "accuracy = correct / total\n",
    "print('Accuracy is {:.1%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), \n",
    "                                  range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.gca().set_xticklabels(class_name_list)\n",
    "    plt.gca().set_yticklabels(class_name_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def create_and_plot_confusion_matrix(actual, predicted):\n",
    "    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(class_name_list)))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=range(len(class_name_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(actual, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
