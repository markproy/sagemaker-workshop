{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Total Loss Based on Structured Claim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Generate a Dataset\n",
    "We will generate a binary classification dataset to represent structured insurance claim data.  We will set two informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title(\"Two informative features, one cluster per class\", fontsize='small')\n",
    "X1, Y1 = make_classification(n_samples=1000, n_features=13, n_redundant=0, n_informative=2,\n",
    "                             n_classes=2, n_clusters_per_class=1, shuffle=False,\n",
    "                             class_sep=2.0)\n",
    "\n",
    "# scatter plot of the first 2 features, highlighting separation of Loss/Not-Loss classes\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "val_size  = 0.20\n",
    "test_size = 0.10\n",
    "\n",
    "# Give 70% to train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y1, \n",
    "                                                    test_size=(test_size + val_size), random_state=seed)\n",
    "\n",
    "# Of the remaining 30%, give 2/3 to validation and 1/3 to test\n",
    "X_test, X_val, y_test, y_val     = train_test_split(X_test, y_test, \n",
    "                                                    test_size=(test_size / (test_size + val_size)), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "mean = X_train.mean(axis=0)\n",
    "X_train -= mean\n",
    "std = X_train.std(axis=0)\n",
    "X_train /= std\n",
    "\n",
    "X_test -= mean\n",
    "X_test /= std\n",
    "\n",
    "X_val -= mean\n",
    "X_val /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train shape: {}, Test shape: {}, Val shape: {}'.format(X_train.shape, \n",
    "                                                              X_test.shape, X_val.shape))\n",
    "print('Train target: {}, Test target: {}, Val target: {}'.format(y_train.shape, \n",
    "                                                                 y_test.shape, y_val.shape))\n",
    "\n",
    "print('\\nSample observation: {}\\nSample target: {}'.format(X_test[0], y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(os.getcwd(), 'local_loss_data')\n",
    "print('Local data dir: {}'.format(data_dir))\n",
    "\n",
    "import pandas as pd\n",
    "xtrain = pd.DataFrame(X_train)\n",
    "xtrain.to_csv(f'{data_dir}/train/xtrain.csv', header=None, index=False)\n",
    "ytrain = pd.DataFrame(y_train)\n",
    "ytrain.to_csv(f'{data_dir}/train/ytrain.csv', header=None, index=False)\n",
    "\n",
    "xtest = pd.DataFrame(X_test)\n",
    "xtest.to_csv(f'{data_dir}/test/xtest.csv', header=None, index=False)\n",
    "ytest = pd.DataFrame(y_test)\n",
    "ytest.to_csv(f'{data_dir}/test/ytest.csv', header=None, index=False)\n",
    "\n",
    "xval = pd.DataFrame(X_val)\n",
    "xval.to_csv(f'{data_dir}/val/xval.csv', header=None, index=False)\n",
    "yval = pd.DataFrame(y_val)\n",
    "yval.to_csv(f'{data_dir}/val/yval.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'scripts/loss_train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "local = False\n",
    "if (local):\n",
    "    train_instance_type = 'local'\n",
    "    serve_instance_type = 'local'\n",
    "else:\n",
    "    train_instance_type = 'ml.c5.xlarge' \n",
    "    serve_instance_type = 'ml.m4.xlarge'\n",
    "\n",
    "hyperparameters = {'epochs': 35, 'data_dir': '/opt/ml/input/data'}\n",
    "\n",
    "loss_estimator = TensorFlow(entry_point='loss_train.py',\n",
    "                       source_dir='scripts',\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       metric_definitions=[\n",
    "                           {'Name' : 'validation:acc', \n",
    "                            'Regex': '.*step.* - val_acc: (\\\\S+)\\n'},\n",
    "                           {'Name' : 'validation:loss', \n",
    "                            'Regex': '- val_loss: (.*?) '}],\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(), # Pass notebook role to container\n",
    "                       framework_version='1.12',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)\n",
    "\n",
    "# In training script, you have to save the model in 'saved model' format to use TF serving\n",
    "#https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (local):\n",
    "    loss_estimator.fit({'train': f'file://{data_dir}/train',\n",
    "                    'test' : f'file://{data_dir}/test',\n",
    "                    'val'  : f'file://{data_dir}/val'}) \n",
    "else:\n",
    "    # upload the files to the s3 bucket\n",
    "    s3_base = sagemaker_session.upload_data(path=data_dir, \n",
    "                                           bucket='roymark-aws-ml',\n",
    "                                           key_prefix='loss')\n",
    "    print(s3_base)\n",
    "    loss_estimator.fit({'train': f'{s3_base}/train',\n",
    "                    'test' : f'{s3_base}/test',\n",
    "                    'val'  : f'{s3_base}/val'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy following script mode training\n",
    "loss_predictor = loss_estimator.deploy(initial_instance_count=1, \n",
    "                                       instance_type=serve_instance_type,\n",
    "                                       endpoint_type='tensorflow-serving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loss_predictor.predict(X_test)\n",
    "print('Results: {}\\n'.format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_results = results['predictions']\n",
    "fail_count = 0\n",
    "test_count = len(X_test)\n",
    "for i in range(test_count):\n",
    "    if (tmp_results[i][0] > 0.5):\n",
    "        class_predict = 1\n",
    "    else:\n",
    "        class_predict = 0\n",
    "    if (class_predict == y_test[i]):\n",
    "        result = 'PASS'\n",
    "    else:\n",
    "        result = '*FAIL'\n",
    "        fail_count += 1\n",
    "    print('Result: {:.3f}, Target: {}, Result: {}'.format(tmp_results[i][0], \n",
    "                                                          y_test[i],\n",
    "                                                         result))\n",
    "print('Tests: {}, Fails: {}'.format(test_count, fail_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    sagemaker.Session().delete_endpoint(loss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
